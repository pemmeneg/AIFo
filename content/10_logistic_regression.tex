\section{Classification \& Logistic Regression}
\subsection{Binary Classification}
\begin{itemize}
    \item Decision with 2 possible outcomes
    \item Hail in Lausanne (yes/no)
    \item Master admission (admission / no admission)
    \item Based on different data / entity
\end{itemize}

\subsubsection{Decision using Linear Regression}
\begin{itemize}
    \item Train the model with gradient descent
    \item \textbf{Bad Idea!}
    \item Models the response (y) and post process the response to compute the probability
\end{itemize}

\subsubsection{The sigmoid function}
\begin{center}
    $sigmoid(y) = \frac{1}{1 + e^{-y}}$
\end{center}
\includegraphics[width=\linewidth]{./img/sigmoid.png}
\textbf{Probabilities}
\begin{itemize}
    \item We can write the estimated probability
    \item For a prediction we can write
\end{itemize}
\begin{center}
    $P(x) = \frac{1}{1 + e^{-(W^{T}x)}}$
\end{center}

\subsubsection{Maximum Likelihood}
\begin{itemize}
    \item Given all the data points (X,Y) we want to maximize the probability that all the predictions are correct.
    \item For each of the training data, we want to maximize the likelihood of correct prediction
    \item We can use Gradient Descent to find W
\end{itemize}
\begin{center}
    \textbf{Maximize Cost:}\\
    \[ Maximum Cost_{2}(W) = \sum_{y=1}^{N} \ln(p(x_{i}) + \sum_{y=0}^{N} \ln(1-p(x_{i})) \]
    \textbf{Loss Function (Minimize Cost):}\\
    \[ Minimize Cost(W) = \frac{-1}{N} \displaystyle\sum_{i = 1}^{N} (y_i * ln(p_i)) + (1 - y_i) * ln(1 - p_i)\]
\end{center}
\section{Optimization}
\begin{itemize}
    \item Training or learning in AI often suggests an algorithm performing some sort of optimization
    \item It is the problem of finding a set of inputs to an objective function that results in a maximum or minimum function evaluation
    \item In our examples the objective is to minimize the loss function
\end{itemize}

\subsection{Gradient Descent}
\begin{itemize}
    \item Iterative Method
    \item Each iteration, the model parameters are updated such as that the Loss (MSE) is reduced
\end{itemize}
\begin{center}
    \textbf{Parametervektor zum Zeitpunkt $t+1$}\\
    $\begin{bmatrix}
        a \\b
    \end{bmatrix}_{t+1} 
    = 
    \begin{bmatrix}
        a\\b
    \end{bmatrix}_{t} - \alpha 
    \begin{bmatrix}
        \frac{\delta E}{\delta a} \\ \frac{\delta E}{\delta b}
    \end{bmatrix}
    | \begin{bmatrix}
        a \\ b
    \end{bmatrix}_{t}$\\
    $\frac{\delta E}{\delta a} = \frac{1}{N} \displaystyle\sum_{i = 1}^{N}(y_i - (ax_i + b))*(-x)$\\
    $\frac{\delta E}{\delta b} = \frac{1}{N} \displaystyle\sum_{i = 1}^{N}(y_i - (ax_i + b))*(-1)$
\end{center}

\subsection{Stochastic Gradient Descent (SGD)}
\begin{itemize}
    \item At each iteration, the gradient is calculated on a (randomly selected) subset of the data
    \item For a fixed learning rate, SGD does not converge
    \item Each itereration, the entire parameter vector gets optimized
    \item Only finds the local Minimum
\end{itemize}

\subsubsection{Annealed SGD}
\begin{itemize}
    \item The learning rate alpha is reduced over time
    \item This is called (simulated) annealing
    \item There are different options (called schedules) how to reduce alpha over time
\end{itemize}

\subsubsection{General remarks on SGD}
\begin{itemize}
    \item Gradient-based methods only work if we can express a Loss function as a differentiable function
    \item SGD is dealing woth only a single datum at each iteration. This is very inefficient and rarely used.
    \item Batch- or mini-batch gradient-descent is usually used
\end{itemize}